{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_R1KOeEr1OM"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <h1 align=\"center\">TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement</h1>\n",
        "  <p align=\"center\">\n",
        "    <a href=\"http://www.carldoersch.com/\">Carl Doersch</a>\n",
        "    ·\n",
        "    <a href=\"https://yangyi02.github.io/\">Yi Yang</a>\n",
        "    ·\n",
        "    <a href=\"https://scholar.google.com/citations?user=Jvi_XPAAAAAJ\">Mel Vecerik</a>\n",
        "    ·\n",
        "    <a href=\"https://scholar.google.com/citations?user=cnbENAEAAAAJ\">Dilara Gokay</a>\n",
        "    ·\n",
        "    <a href=\"https://www.robots.ox.ac.uk/~ankush/\">Ankush Gupta</a>\n",
        "    ·\n",
        "    <a href=\"http://people.csail.mit.edu/yusuf/\">Yusuf Aytar</a>\n",
        "    ·\n",
        "    <a href=\"https://scholar.google.co.uk/citations?user=IUZ-7_cAAAAJ\">Joao Carreira</a>\n",
        "    ·\n",
        "    <a href=\"https://www.robots.ox.ac.uk/~az/\">Andrew Zisserman</a>\n",
        "  </p>\n",
        "  <h3 align=\"center\"><a href=\"https://arxiv.org/abs/2306.08637\">Paper</a> | <a href=\"https://deepmind-tapir.github.io\">Project Page</a> | <a href=\"https://github.com/deepmind/tapnet\">GitHub</a> | <a href=\"https://github.com/deepmind/tapnet/tree/main#running-tapir-locally\">Live Demo</a> </h3>\n",
        "  <div align=\"center\"></div>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://storage.googleapis.com/dm-tapnet/horsejump_rainbow.gif\" width=\"70%\"/><br/><br/>\n",
        "</p>\n",
        "<p>\n",
        "  This visualization uses TAPIR to show how an object moves through space, even if the camera is tracking the object.  It begins by tracking points densely on a grid.  Then it estimates the camera motion as a homography (i.e., assuming either planar background or camera that rotates but does not move).  Any points that move according to that homography are removed.  Then we generate a &ldquo;rainbow&rdquo; visualization, where the tracked points leave &ldquo;tails&rdquo; that follow the camera motion, so it looks like the earlier positions of points are frozen in space.  This visualization was inspired by a similar one from <a href=\"https://omnimotion.github.io/\">OmniMotion</a>, although that one assumes ground-truth segmentations are available and models the camera as only 2D translation.\n",
        "</p>\n",
        "<p>\n",
        "  Note that we consider this algorithm &ldquo;semi-automatic&rdquo; because you may need some tuning for pleasing results on arbitrary videos.  Tracking failures on the background may show up as foreground objects.  Results are sensitive to the outlier thresholds used in RANSAC and segmentation, and you may wish to discard short tracks.  You can sample in a different way (e.g. sampling points from multiple frames) and everything will work, but the <font face=\"Courier\">plot_tracks_tails</font> function uses the input order of the points to choose colors, so you will have to sort the points appropriately.\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yflCqOMaDJP"
      },
      "outputs": [],
      "source": [
        "# @title Download Code {form-width: \"25%\"}\n",
        "!git clone https://github.com/deepmind/tapnet.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KmxmZ9xaFWC"
      },
      "outputs": [],
      "source": [
        "# @title Install Dependencies {form-width: \"25%\"}\n",
        "!pip install -r tapnet/requirements_inference.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnG2QEbxaH5Q"
      },
      "outputs": [],
      "source": [
        "# @title Download Model {form-width: \"25%\"}\n",
        "\n",
        "%mkdir tapnet/checkpoints\n",
        "\n",
        "!wget -P tapnet/checkpoints https://storage.googleapis.com/dm-tapnet/tapir_checkpoint_panning.npy\n",
        "\n",
        "%ls tapnet/checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOHqUSIsmmd0"
      },
      "outputs": [],
      "source": [
        "# @title Imports {form-width: \"25%\"}\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import haiku as hk\n",
        "import mediapy as media\n",
        "import numpy as np\n",
        "import tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK7F5PHdsBZ0"
      },
      "outputs": [],
      "source": [
        "from tapnet import tapir_model\n",
        "from tapnet.utils import transforms\n",
        "from tapnet.utils import viz_utils\n",
        "from tapnet.utils import model_utils\n",
        "\n",
        "# @title Load Checkpoint {form-width: \"25%\"}\n",
        "\n",
        "checkpoint_path = 'tapnet/checkpoints/tapir_checkpoint_panning.npy'\n",
        "ckpt_state = np.load(checkpoint_path, allow_pickle=True).item()\n",
        "params, state = ckpt_state['params'], ckpt_state['state']\n",
        "\n",
        "# @title Build Model {form-width: \"25%\"}\n",
        "\n",
        "def build_model(frames, query_points):\n",
        "  \"\"\"Compute point tracks and occlusions given frames and query points.\"\"\"\n",
        "  model = tapir_model.TAPIR(bilinear_interp_with_depthwise_conv=False, pyramid_level=0)\n",
        "  outputs = model(\n",
        "      video=frames,\n",
        "      is_training=False,\n",
        "      query_points=query_points,\n",
        "      query_chunk_size=64,\n",
        "  )\n",
        "  return outputs\n",
        "\n",
        "model = hk.transform_with_state(build_model)\n",
        "model_apply = jax.jit(model.apply)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64dc5wE7KkC-"
      },
      "source": [
        "## Load and Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNn1sLNaeST8"
      },
      "outputs": [],
      "source": [
        "# @title Inference function {form-width: \"25%\"}\n",
        "\n",
        "def inference(frames, query_points):\n",
        "  \"\"\"Inference on one video.\n",
        "\n",
        "  Args:\n",
        "    frames: [num_frames, height, width, 3], [0, 255], np.uint8\n",
        "    query_points: [num_points, 3], [0, num_frames/height/width], [t, y, x]\n",
        "\n",
        "  Returns:\n",
        "    tracks: [num_points, 3], [-1, 1], [t, y, x]\n",
        "    visibles: [num_points, num_frames], bool\n",
        "  \"\"\"\n",
        "  # Preprocess video to match model inputs format\n",
        "  frames = model_utils.preprocess_frames(frames)\n",
        "  num_frames, height, width = frames.shape[0:3]\n",
        "  query_points = query_points.astype(np.float32)\n",
        "  frames, query_points = frames[None], query_points[None]  # Add batch dimension\n",
        "\n",
        "  # Model inference\n",
        "  rng = jax.random.PRNGKey(42)\n",
        "  outputs, _ = model_apply(params, state, rng, frames, query_points)\n",
        "  outputs = tree.map_structure(lambda x: np.array(x[0]), outputs)\n",
        "  tracks, occlusions, expected_dist = outputs['tracks'], outputs['occlusion'], outputs['expected_dist']\n",
        "\n",
        "  # Binarize occlusions\n",
        "  visibles = model_utils.postprocess_occlusions(occlusions, expected_dist)\n",
        "  return tracks, visibles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2daaUz12gnE"
      },
      "outputs": [],
      "source": [
        "# @title Utilities for model inference {form-width: \"25%\"}\n",
        "\n",
        "def sample_grid_points(frame_idx, height, width, stride=1):\n",
        "  \"\"\"Sample grid points with (time height, width) order.\"\"\"\n",
        "  points = np.mgrid[stride//2:height:stride, stride//2:width:stride]\n",
        "  points = points.transpose(1, 2, 0)\n",
        "  out_height, out_width = points.shape[0:2]\n",
        "  frame_idx = np.ones((out_height, out_width, 1)) * frame_idx\n",
        "  points = np.concatenate((frame_idx, points), axis=-1).astype(np.int32)\n",
        "  points = points.reshape(-1, 3)  # [out_height*out_width, 3]\n",
        "  return points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTeDYLaRE2zs"
      },
      "outputs": [],
      "source": [
        "# @title Load an Exemplar Video {form-width: \"25%\"}\n",
        "\n",
        "%mkdir tapnet/examplar_videos\n",
        "\n",
        "!wget -P tapnet/examplar_videos https://storage.googleapis.com/dm-tapnet/horsejump-high.mp4\n",
        "\n",
        "orig_frames = media.read_video('tapnet/examplar_videos/horsejump-high.mp4')\n",
        "height, width = orig_frames.shape[1:3]\n",
        "media.show_video(orig_frames, fps=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5A-j_PWUFmA"
      },
      "outputs": [],
      "source": [
        "# @title Predict semi-dense point tracks {form-width: \"25%\"}\n",
        "%%time\n",
        "\n",
        "resize_height = 512  # @param {type: \"integer\"}\n",
        "resize_width = 512  # @param {type: \"integer\"}\n",
        "stride = 16  # @param {type: \"integer\"}\n",
        "query_frame = 0  # @param {type: \"integer\"}\n",
        "\n",
        "height, width = orig_frames.shape[1:3]\n",
        "frames = media.resize_video(orig_frames, (resize_height, resize_width))\n",
        "query_points = sample_grid_points(query_frame, resize_height, resize_width, stride)\n",
        "batch_size = 64\n",
        "tracks = []\n",
        "visibles = []\n",
        "for i in range(0,query_points.shape[0],batch_size):\n",
        "  query_points_chunk = query_points[i:i+batch_size]\n",
        "  num_extra = batch_size - query_points_chunk.shape[0]\n",
        "  if num_extra > 0:\n",
        "    query_points_chunk = np.concatenate([query_points_chunk, np.zeros([num_extra, 3])], axis=0)\n",
        "  tracks2, visibles2 = inference(frames, query_points_chunk)\n",
        "  if num_extra > 0:\n",
        "    tracks2 = tracks2[:-num_extra]\n",
        "    visibles2 = visibles2[:-num_extra]\n",
        "  tracks.append(tracks2)\n",
        "  visibles.append(visibles2)\n",
        "tracks=jnp.concatenate(tracks, axis=0)\n",
        "visibles=jnp.concatenate(visibles, axis=0)\n",
        "\n",
        "tracks = transforms.convert_grid_coordinates(tracks, (resize_width, resize_height), (width, height))\n",
        "\n",
        "# We show the point tracks without rainbows so you can see the input.\n",
        "video = viz_utils.plot_tracks_v2(orig_frames, tracks, 1.0 - visibles)\n",
        "media.show_video(video, fps=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vyl_hSxaJFsz"
      },
      "outputs": [],
      "source": [
        "# The inlier point threshold for ransac, specified in normalized coordinates\n",
        "# (points are rescaled to the range [0, 1] for optimization).\n",
        "ransac_inlier_threshold = 0.07  # @param {type: \"number\"}\n",
        "# What fraction of points need to be inliers for RANSAC to consider a trajectory\n",
        "# to be trustworthy for estimating the homography.\n",
        "ransac_track_inlier_frac = 0.95  # @param {type: \"number\"}\n",
        "# After initial RANSAC, how many refinement passes to adjust the homographies\n",
        "# based on tracks that have been deemed trustworthy.\n",
        "num_refinement_passes = 2  # @param {type: \"number\"}\n",
        "# After homographies are estimated, consider points to be outliers if they are\n",
        "# further than this threshold.\n",
        "foreground_inlier_threshold = 0.07  # @param {type: \"number\"}\n",
        "# After homographies are estimated, consider tracks to be part of the foreground\n",
        "# if less than this fraction of its points are inliers.\n",
        "foreground_frac = 0.6  # @param {type: \"number\"}\n",
        "\n",
        "\n",
        "occluded = 1.0 - visibles\n",
        "homogs, err, canonical = viz_utils.get_homographies_wrt_frame(\n",
        "    tracks,\n",
        "    occluded,\n",
        "    [width, height],\n",
        "    thresh=ransac_inlier_threshold,\n",
        "    outlier_point_threshold=ransac_track_inlier_frac,\n",
        "    num_refinement_passes=num_refinement_passes,\n",
        ")\n",
        "\n",
        "inliers = (err < np.square(foreground_inlier_threshold)) * visibles\n",
        "inlier_ct = np.sum(inliers, axis=-1)\n",
        "ratio = inlier_ct / np.maximum(1.0, np.sum(visibles, axis=1))\n",
        "is_fg = ratio <= foreground_frac\n",
        "video = viz_utils.plot_tracks_tails(\n",
        "    orig_frames,\n",
        "    tracks[is_fg],\n",
        "    occluded[is_fg],\n",
        "    homogs\n",
        ")\n",
        "media.show_video(video, fps=24)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
