{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_R1KOeEr1OM"
      },
      "source": [
        "\u003cp align=\"center\"\u003e\n",
        "  \u003ch1 align=\"center\"\u003eTAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement\u003c/h1\u003e\n",
        "  \u003cp align=\"center\"\u003e\n",
        "    \u003ca href=\"http://www.carldoersch.com/\"\u003eCarl Doersch\u003c/a\u003e\n",
        "    ·\n",
        "    \u003ca href=\"https://yangyi02.github.io/\"\u003eYi Yang\u003c/a\u003e\n",
        "    ·\n",
        "    \u003ca href=\"https://scholar.google.com/citations?user=Jvi_XPAAAAAJ\"\u003eMel Vecerik\u003c/a\u003e\n",
        "    ·\n",
        "    \u003ca href=\"https://scholar.google.com/citations?user=cnbENAEAAAAJ\"\u003eDilara Gokay\u003c/a\u003e\n",
        "    ·\n",
        "    \u003ca href=\"https://www.robots.ox.ac.uk/~ankush/\"\u003eAnkush Gupta\u003c/a\u003e\n",
        "    ·\n",
        "    \u003ca href=\"http://people.csail.mit.edu/yusuf/\"\u003eYusuf Aytar\u003c/a\u003e\n",
        "    ·\n",
        "    \u003ca href=\"https://scholar.google.co.uk/citations?user=IUZ-7_cAAAAJ\"\u003eJoao Carreira\u003c/a\u003e\n",
        "    ·\n",
        "    \u003ca href=\"https://www.robots.ox.ac.uk/~az/\"\u003eAndrew Zisserman\u003c/a\u003e\n",
        "  \u003c/p\u003e\n",
        "  \u003ch3 align=\"center\"\u003e\u003ca href=\"https://arxiv.org/abs/2306.08637\"\u003ePaper\u003c/a\u003e | \u003ca href=\"https://deepmind-tapir.github.io\"\u003eProject Page\u003c/a\u003e | \u003ca href=\"https://github.com/deepmind/tapnet\"\u003eGitHub\u003c/a\u003e | \u003ca href=\"https://github.com/deepmind/tapnet/tree/main#running-tapir-locally\"\u003eLive Demo\u003c/a\u003e \u003c/h3\u003e\n",
        "  \u003cdiv align=\"center\"\u003e\u003c/div\u003e\n",
        "\u003c/p\u003e\n",
        "\n",
        "\u003cp align=\"center\"\u003e\n",
        "  \u003cimg src=\"https://storage.googleapis.com/dm-tapnet/horsejump_rainbow.gif\" width=\"70%\"/\u003e\n",
        "\u003c/p\u003e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yflCqOMaDJP"
      },
      "outputs": [],
      "source": [
        "# @title Download Code {form-width: \"25%\"}\n",
        "!git clone https://github.com/deepmind/tapnet.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KmxmZ9xaFWC"
      },
      "outputs": [],
      "source": [
        "# @title Install Dependencies {form-width: \"25%\"}\n",
        "!pip install -r tapnet/requirements_inference.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnG2QEbxaH5Q"
      },
      "outputs": [],
      "source": [
        "# @title Download Model {form-width: \"25%\"}\n",
        "\n",
        "%mkdir tapnet/checkpoints\n",
        "\n",
        "!wget -P tapnet/checkpoints https://storage.googleapis.com/dm-tapnet/tapir_checkpoint_panning.npy\n",
        "\n",
        "%ls tapnet/checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOHqUSIsmmd0"
      },
      "outputs": [],
      "source": [
        "# @title Imports {form-width: \"25%\"}\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import haiku as hk\n",
        "import mediapy as media\n",
        "import numpy as np\n",
        "import tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK7F5PHdsBZ0"
      },
      "outputs": [],
      "source": [
        "from tapnet import tapir_model\n",
        "from tapnet.utils import transforms\n",
        "from tapnet.utils import viz_utils\n",
        "from tapnet.utils import model_utils\n",
        "\n",
        "# @title Load Checkpoint {form-width: \"25%\"}\n",
        "\n",
        "checkpoint_path = 'tapnet/checkpoints/tapir_checkpoint_panning.npy'\n",
        "ckpt_state = np.load(checkpoint_path, allow_pickle=True).item()\n",
        "params, state = ckpt_state['params'], ckpt_state['state']\n",
        "\n",
        "# @title Build Model {form-width: \"25%\"}\n",
        "\n",
        "def build_model(frames, query_points):\n",
        "  \"\"\"Compute point tracks and occlusions given frames and query points.\"\"\"\n",
        "  model = tapir_model.TAPIR(bilinear_interp_with_depthwise_conv=False, pyramid_level=0)\n",
        "  outputs = model(\n",
        "      video=frames,\n",
        "      is_training=False,\n",
        "      query_points=query_points,\n",
        "      query_chunk_size=64,\n",
        "  )\n",
        "  return outputs\n",
        "\n",
        "model = hk.transform_with_state(build_model)\n",
        "model_apply = jax.jit(model.apply)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64dc5wE7KkC-"
      },
      "source": [
        "## Load and Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNn1sLNaeST8"
      },
      "outputs": [],
      "source": [
        "# @title Inference function {form-width: \"25%\"}\n",
        "\n",
        "def inference(frames, query_points):\n",
        "  \"\"\"Inference on one video.\n",
        "\n",
        "  Args:\n",
        "    frames: [num_frames, height, width, 3], [0, 255], np.uint8\n",
        "    query_points: [num_points, 3], [0, num_frames/height/width], [t, y, x]\n",
        "\n",
        "  Returns:\n",
        "    tracks: [num_points, 3], [-1, 1], [t, y, x]\n",
        "    visibles: [num_points, num_frames], bool\n",
        "  \"\"\"\n",
        "  # Preprocess video to match model inputs format\n",
        "  frames = model_utils.preprocess_frames(frames)\n",
        "  num_frames, height, width = frames.shape[0:3]\n",
        "  query_points = query_points.astype(np.float32)\n",
        "  frames, query_points = frames[None], query_points[None]  # Add batch dimension\n",
        "\n",
        "  # Model inference\n",
        "  rng = jax.random.PRNGKey(42)\n",
        "  outputs, _ = model_apply(params, state, rng, frames, query_points)\n",
        "  outputs = tree.map_structure(lambda x: np.array(x[0]), outputs)\n",
        "  tracks, occlusions, expected_dist = outputs['tracks'], outputs['occlusion'], outputs['expected_dist']\n",
        "\n",
        "  # Binarize occlusions\n",
        "  visibles = model_utils.postprocess_occlusions(occlusions, expected_dist)\n",
        "  return tracks, visibles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2daaUz12gnE"
      },
      "outputs": [],
      "source": [
        "# @title Utilities for model inference {form-width: \"25%\"}\n",
        "\n",
        "def sample_grid_points(frame_idx, height, width, stride=1):\n",
        "  \"\"\"Sample grid points with (time height, width) order.\"\"\"\n",
        "  points = np.mgrid[stride//2:height:stride, stride//2:width:stride]\n",
        "  points = points.transpose(1, 2, 0)\n",
        "  out_height, out_width = points.shape[0:2]\n",
        "  frame_idx = np.ones((out_height, out_width, 1)) * frame_idx\n",
        "  points = np.concatenate((frame_idx, points), axis=-1).astype(np.int32)\n",
        "  points = points.reshape(-1, 3)  # [out_height*out_width, 3]\n",
        "  return points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7X5ZNCpuemg"
      },
      "source": [
        "## Inference on DAVIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTeDYLaRE2zs"
      },
      "outputs": [],
      "source": [
        "# @title Load an Exemplar Video {form-width: \"25%\"}\n",
        "\n",
        "%mkdir tapnet/examplar_videos\n",
        "\n",
        "!wget -P tapnet/examplar_videos https://storage.googleapis.com/dm-tapnet/horsejump-high.mp4\n",
        "\n",
        "orig_frames = media.read_video('tapnet/examplar_videos/horsejump-high.mp4')\n",
        "height, width = orig_frames.shape[1:3]\n",
        "media.show_video(orig_frames, fps=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5A-j_PWUFmA"
      },
      "outputs": [],
      "source": [
        "# @title Predict semi-dense point tracks {form-width: \"25%\"}\n",
        "%%time\n",
        "\n",
        "resize_height = 512  # @param {type: \"integer\"}\n",
        "resize_width = 512  # @param {type: \"integer\"}\n",
        "stride = 16  # @param {type: \"integer\"}\n",
        "\n",
        "height, width = orig_frames.shape[1:3]\n",
        "frames = media.resize_video(orig_frames, (resize_height, resize_width))\n",
        "query_points = sample_grid_points(0, resize_height, resize_width, stride)\n",
        "batch_size = 64\n",
        "tracks = []\n",
        "visibles = []\n",
        "for i in range(0,query_points.shape[0],batch_size):\n",
        "  query_points_chunk = query_points[i:i+batch_size]\n",
        "  num_extra = batch_size - query_points_chunk.shape[0]\n",
        "  if num_extra \u003e 0:\n",
        "    query_points_chunk = np.concatenate([query_points_chunk, np.zeros([num_extra, 3])], axis=0)\n",
        "  tracks2, visibles2 = inference(frames, query_points_chunk)\n",
        "  if num_extra \u003e 0:\n",
        "    tracks2 = tracks2[:-num_extra]\n",
        "    visibles2 = visibles2[:-num_extra]\n",
        "  tracks.append(tracks2)\n",
        "  visibles.append(visibles2)\n",
        "tracks=jnp.concatenate(tracks, axis=0)\n",
        "visibles=jnp.concatenate(visibles, axis=0)\n",
        "\n",
        "tracks = transforms.convert_grid_coordinates(tracks, (resize_width, resize_height), (width, height))\n",
        "\n",
        "# We show the point tracks without rainbows so you can see the input.\n",
        "video = viz_utils.plot_tracks_v2(orig_frames, tracks, 1.0 - visibles)\n",
        "media.show_video(video, fps=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vyl_hSxaJFsz"
      },
      "outputs": [],
      "source": [
        "occluded = 1.0 - visibles\n",
        "homogs, err, canonical = viz_utils.get_homographies_wrt_frame(\n",
        "    tracks,\n",
        "    occluded,\n",
        "    [width, height]\n",
        ")\n",
        "\n",
        "# sort by position in canonical frame.  In this demo they're already essentially\n",
        "# sorted, but if you query points from multiple frames or are chosen randomly,\n",
        "# they won't be.\n",
        "ordr = np.argsort(canonical[:,1])\n",
        "sorted_tracks = tracks[ordr]\n",
        "sorted_occ = occluded[ordr]\n",
        "sorted_err = err[ordr]\n",
        "inlier_ct = np.sum((sorted_err \u003c np.square(0.07)) * (1 - sorted_occ), axis=-1)\n",
        "ratio = inlier_ct / np.maximum(1.0, np.sum(1 - sorted_occ, axis=1))\n",
        "is_fg = ratio \u003c= 0.60\n",
        "video = viz_utils.plot_tracks_tails(\n",
        "    orig_frames,\n",
        "    sorted_tracks[is_fg],\n",
        "    sorted_occ[is_fg],\n",
        "    homogs\n",
        ")\n",
        "media.show_video(video, fps=16)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
